import os
import torch
import torchaudio
import torchaudio.transforms as T
from transformers import AutoProcessor, AutoModel
import torch.nn as nn

# ì „ì—­ ì„¤ì •
THRESHOLD = 0.1
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
MODEL_PATH = os.path.join(BASE_DIR, "whisper_dog_model.pth")


# Whisper ë¶„ë¥˜ê¸° ì •ì˜
class WhisperClassifier(nn.Module):
    def __init__(self, base_model, hidden_dim=768, num_labels=2):
        super().__init__()
        self.encoder = base_model.encoder
        self.classifier = nn.Sequential(
            nn.Linear(hidden_dim, 128),
            nn.ReLU(),
            nn.Linear(128, num_labels)
        )

    def forward(self, input_features):
        with torch.no_grad():
            features = self.encoder(input_features).last_hidden_state
        pooled = features.mean(dim=1)
        return self.classifier(pooled)

# Whisper ëª¨ë¸ ë° Processor ë¡œë“œ
processor = AutoProcessor.from_pretrained("openai/whisper-small")
base_model = AutoModel.from_pretrained("openai/whisper-small")
model = WhisperClassifier(base_model).to(device)

# ì €ì¥ëœ í•™ìŠµ íŒŒë¼ë¯¸í„° ë¡œë“œ
model.load_state_dict(torch.load(MODEL_PATH, map_location=device))
model.eval()

# ì „ì²˜ë¦¬ í•¨ìˆ˜
def preprocess_audio(path, silence_threshold=0.01):
    waveform, sr = torchaudio.load(path)
    waveform = waveform.mean(dim=0).unsqueeze(0)  # mono

    # ë¬´ìŒ í•„í„°
    if waveform.abs().mean().item() < silence_threshold:
        return None

    # ë¦¬ìƒ˜í”Œë§
    if sr != 16000:
        waveform = T.Resample(orig_freq=sr, new_freq=16000)(waveform)

    # ì •ê·œí™”
    waveform = waveform / waveform.abs().max()

    # Whisper ì…ë ¥
    inputs = processor(waveform.squeeze(), sampling_rate=16000, return_tensors="pt")
    return inputs["input_features"].squeeze(0).unsqueeze(0).to(device)

# ê°ì§€ í•¨ìˆ˜: ì…ë ¥ â†’ ê²°ê³¼(True/False), confidence ë°˜í™˜
def detect_bark(file_path, threshold=0.1):
    try:
        input_features = preprocess_audio(file_path)
        if input_features is None:
            return False, 0.0

        with torch.no_grad():
            logits = model(input_features)
            probs = torch.softmax(logits, dim=1)[0]
            dog_prob = probs[1].item()

        return (dog_prob >= threshold), dog_prob

    except Exception:
        return False, 0.0

# CLI í…ŒìŠ¤íŠ¸ìš©
if __name__ == "__main__":
    import sys
    if len(sys.argv) != 2:
        print("ì‚¬ìš©ë²•: python barknet_detector.py <ì˜¤ë””ì˜¤íŒŒì¼ê²½ë¡œ>")
    else:
        result, conf = detect_bark(sys.argv[1])
        print("ğŸ—£ï¸ ê°œê°€ ì§–ì—ˆìŠµë‹ˆë‹¤!" if result else "ğŸ”‡ ì§–ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", f"(ì‹ ë¢°ë„: {conf:.2f})")
